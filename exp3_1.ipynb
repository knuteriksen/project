{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from optuna.integration import SkoptSampler\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.suggest.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config6 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": tune.quniform(2, 10, 2),\n",
    "    \"hidden_layer_width\": tune.quniform(30, 120, 15),\n",
    "    \"dropout\": tune.uniform(0.0, 0.4),\n",
    "    \"l2\": tune.loguniform(1e-6, 1e-1),\n",
    "}\n",
    "\n",
    "config5 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": tune.quniform(2, 10, 2),\n",
    "    \"hidden_layer_width\": tune.quniform(30, 120, 15),\n",
    "    \"dropout\": tune.uniform(0.0, 0.4),\n",
    "    \"l2\": 1e-3,\n",
    "}\n",
    "\n",
    "config4 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": tune.quniform(2, 10, 2),\n",
    "    \"hidden_layer_width\": tune.quniform(30, 120, 15),\n",
    "    \"dropout\": tune.uniform(0.0, 0.4),\n",
    "    \"l2\": 1e-3,\n",
    "}\n",
    "\n",
    "config3 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": tune.quniform(2, 10, 2),\n",
    "    \"hidden_layer_width\": tune.quniform(30, 120, 15),\n",
    "    \"dropout\": 0.1,\n",
    "    \"l2\": 1e-3,\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": tune.quniform(2, 10, 2),\n",
    "    \"hidden_layer_width\": 75,\n",
    "    \"dropout\": 0.1,\n",
    "    \"l2\": 1e-3,\n",
    "}\n",
    "\n",
    "config1 = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"hidden_layers\": 6,\n",
    "    \"hidden_layer_width\": 70,\n",
    "    \"dropout\": 0.1,\n",
    "    \"l2\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 12345\n",
    "ins = ['CHK', 'PWH', 'PDC', 'TWH', 'FGAS', 'FOIL']\n",
    "outs = ['QTOT']\n",
    "\n",
    "logdir_6 = \"run_results/run6\"\n",
    "logdir_5 = \"run_results/run5\"\n",
    "logdir_4 = \"run_results/run4\"\n",
    "logdir_3 = \"run_results/run3\"\n",
    "logdir_2 = \"run_results/run2\"\n",
    "logdir_1 = \"run_results/run1\"\n",
    "\n",
    "metric = \"mean_square_error\"\n",
    "mode = \"min\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = \"dataset/training_set.csv\"\n",
    "train_set = pd.read_csv(path, index_col=0)\n",
    "path = \"dataset/validation_set.csv\"\n",
    "val_set = pd.read_csv(path, index_col=0)\n",
    "path = \"dataset/test_set.csv\"\n",
    "test_set = pd.read_csv(path, index_col=0)\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        input_cols: [],\n",
    "        output_cols: [],\n",
    "        train_batch_size: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the dataset to be used for HPO\n",
    "    Converts to torch tensors and dataset loaders\n",
    "    :param input_cols: list of strings\n",
    "    :param output_cols: list of strings\n",
    "    :param train_batch_size: Batch size\n",
    "    :return:\n",
    "    :return: train_loader, x_val, y_val, val_loader, x_test, y_test\n",
    "    \"\"\"\n",
    "    # INPUT_COLS = ['CHK', 'PWH', 'PDC', 'TWH', 'FGAS', 'FOIL']\n",
    "    # OUTPUT_COLS = ['QTOT']\n",
    "\n",
    "    # Get input and output tensors and convert them to torch tensors\n",
    "    x_train = torch.from_numpy(train_set[input_cols].values).to(torch.float)\n",
    "    y_train = torch.from_numpy(train_set[output_cols].values).to(torch.float)\n",
    "\n",
    "    x_val = torch.from_numpy(val_set[input_cols].values).to(torch.float)\n",
    "    y_val = torch.from_numpy(val_set[output_cols].values).to(torch.float)\n",
    "\n",
    "    # Create dataset loaders\n",
    "    # Here we specify the batch size and if the dataset should be shuffled\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_set), shuffle=False)\n",
    "\n",
    "    # Get input and output as torch tensors\n",
    "    x_test = torch.from_numpy(test_set[input_cols].values).to(torch.float)\n",
    "    y_test = torch.from_numpy(test_set[output_cols].values).to(torch.float)\n",
    "\n",
    "    return train_loader, x_val, y_val, val_loader, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inputs: int,\n",
    "            hidden_layers: int,\n",
    "            hidden_layer_width: int,\n",
    "            outputs: int,\n",
    "            dropout_value: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dropout_value: Dropout value to use. 0.0 If no dropout is desired\n",
    "        :param inputs: Number of inputs\n",
    "        :param hidden_layers: Number of hidden layers\n",
    "        :param hidden_layer_width: Size of hidden layer\n",
    "        :param outputs: Number of outputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        layers = [inputs] + [hidden_layer_width] * hidden_layers + [outputs]\n",
    "\n",
    "        assert len(layers) >= 2, \"At least two layers are required (incl. input and output layer)\"\n",
    "        self.layers = layers\n",
    "\n",
    "        # Fully connected linear layers\n",
    "        linear_layers = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            n_in = self.layers[i]\n",
    "            n_out = self.layers[i + 1]\n",
    "            layer = torch.nn.Linear(n_in, n_out)\n",
    "\n",
    "            # Initialize weights and biases\n",
    "            a = 1 if i == 0 else 2\n",
    "            layer.weight.data = torch.randn((n_out, n_in)) * np.sqrt(a / n_in)\n",
    "            layer.bias.data = torch.zeros(n_out)\n",
    "\n",
    "            # Add to list\n",
    "            linear_layers.append(layer)\n",
    "\n",
    "            # Add possible dropout\n",
    "            if dropout_value:\n",
    "                linear_layers.append(torch.nn.Dropout(dropout_value))\n",
    "\n",
    "        # Modules/layers must be registered to enable saving of notebooks\n",
    "        self.linear_layers = torch.nn.ModuleList(linear_layers)\n",
    "\n",
    "        # Non-linearity (e.g. ReLU, ELU, or SELU)\n",
    "        self.act = torch.nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass to evaluate network for input values\n",
    "        :param input: tensor assumed to be of size (batch_size, n_inputs)\n",
    "        :return: output tensor\n",
    "        \"\"\"\n",
    "        x = input\n",
    "        for l in self.linear_layers[:-1]:\n",
    "            x = l(x)\n",
    "            x = self.act(x)\n",
    "\n",
    "        output_layer = self.linear_layers[-1]\n",
    "        return output_layer(x)\n",
    "\n",
    "    def get_num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"\n",
    "        Save notebooks state\n",
    "        :param path: Path to save notebooks state\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"\n",
    "        Load notebooks state from file\n",
    "        :param path: Path to saved notebooks state\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    :param config: hyperparameter configuration\n",
    "    :param checkpoint_dir: local checkpoint dir. Leave blank to use ~/ray_results\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    net = Net(\n",
    "        len(ins),\n",
    "        int(config[\"hidden_layers\"]),\n",
    "        int(config[\"hidden_layer_width\"]),\n",
    "        len(outs),\n",
    "        dropout_value=config[\"dropout\"]\n",
    "    )\n",
    "\n",
    "    net = net.to(net.device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint\n",
    "    # should be restored.\n",
    "    if checkpoint_dir:\n",
    "        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state, optimizer_state = torch.load(checkpoint)\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # Import training, validation and test data\n",
    "    train_loader, x_valid, y_valid, val_loader, x_test, y_test = prepare_data(\n",
    "        input_cols=ins,\n",
    "        output_cols=outs,\n",
    "        train_batch_size=64\n",
    "    )\n",
    "\n",
    "    # Train Network\n",
    "    for epoch in range(100):\n",
    "        # specify that we are in training mode\n",
    "        net.train()\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(net.device), labels.to(net.device)\n",
    "            # Zero the parameter gradients (from last iteration)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute cost function\n",
    "            batch_mse = criterion(outputs, labels)\n",
    "\n",
    "            reg_loss = 0\n",
    "            for param in net.parameters():\n",
    "                reg_loss += param.pow(2).sum()\n",
    "\n",
    "            cost = batch_mse + config[\"l2\"] * reg_loss\n",
    "\n",
    "            # Backward propagation to compute gradient\n",
    "            cost.backward()\n",
    "\n",
    "            # Update parameters using gradient\n",
    "            optimizer.step()\n",
    "\n",
    "        # Specify that we are in evaluation mode\n",
    "        net.eval()\n",
    "\n",
    "        # Evaluate model on validation data\n",
    "        mse_val = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(net.device), labels.to(net.device)\n",
    "            mse_val += torch.sum(torch.pow(labels - net(inputs), 2)).item()\n",
    "        mse_val /= len(val_loader.dataset)\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n",
    "        # parameter in future iterations.\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save(\n",
    "                (net.state_dict(), optimizer.state_dict()), path)\n",
    "        tune.report(mean_square_error=mse_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(config: {}, iterations: int, experiment_name: str, logdir: str):\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    sampler = SkoptSampler(\n",
    "        skopt_kwargs={\n",
    "            \"base_estimator\": \"GP\",\n",
    "            \"n_initial_points\": 5,\n",
    "            \"acq_func\": \"EI\",\n",
    "            \"acq_func_kwargs\": {\"xi\": 0.05}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    algo = OptunaSearch(\n",
    "        sampler=sampler,\n",
    "        metric=metric,\n",
    "        mode=mode,\n",
    "    )\n",
    "    algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train),\n",
    "        name=experiment_name,\n",
    "        config=config,\n",
    "        metric=metric,\n",
    "        mode=mode,\n",
    "        search_alg=algo,\n",
    "        num_samples=iterations,\n",
    "        verbose=1,\n",
    "        checkpoint_score_attr=\"min-mean_square_error\",\n",
    "        keep_checkpoints_num=2,\n",
    "        local_dir=logdir,\n",
    "        resources_per_trial={\"cpu\": 1, \"gpu\": 0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    for i in range(0, 8):\n",
    "        print(\"Starting New Experiment\")\n",
    "        experiment_name = \"xp_\" + str(i).rjust(3, \"0\")\n",
    "        optimize(\n",
    "            config=config3,\n",
    "            iterations=100,\n",
    "            experiment_name=experiment_name,\n",
    "            logdir=logdir_3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-20 17:03:32 (running for 00:48:39.74)<br>Memory usage on this node: 9.0/31.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/14.55 GiB heap, 0.0/7.27 GiB objects<br>Current best trial: 5f9e85a2 with mean_square_error=59.95016902043269 and parameters={'lr': 0.09999999999999999, 'hidden_layers': 2.0, 'hidden_layer_width': 50.0, 'dropout': 1, 'batch_size': 16, 'l2': 0.01}<br>Result logdir: /home/knut/Documents/project/run_results/run3/xp_007<br>Number of trials: 100/100 (100 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 17:03:32,900\tINFO tune.py:630 -- Total run time: 2919.89 seconds (2919.73 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}